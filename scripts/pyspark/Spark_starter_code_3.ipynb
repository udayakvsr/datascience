{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92e43e4b-eb97-4789-be40-8b16a5024133",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DS-610 Week 3 Homework: Hive on Databricks\n",
    "This week we will look at data can be manipulated on Apache Hive. As an example, we will look at a part of Walmart's daily stock prices.\n",
    "\n",
    "For this assignment, it is highly recommended that you try it on Saint Peters' Databricks system to get a feel of how you can retrieve data from Hive. We will perform aggregations using the following three methods:\n",
    "1. `groupBy` then `agg` syntax on the loaded DataFrame.\n",
    "2. Creating a table view via `createOrReplaceTempView` then running SQL query on the table view.\n",
    "3. `groupBy` then `agg` with a slightly complicated aggregation logic using a UDF.\n",
    "\n",
    "First we start by loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7423708e-a1a9-4f56-8a0c-af1eb77f1936",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac2f79a-e02a-4759-b180-f74fddf3d4d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Source\n",
    "By default the assignment is designed to run on Saint Peters' Databricks cluster. If you are running on your laptop, uncomment the line below. You also need to download the data file on the Blackboard website into the folder containing this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66bf3b97-855b-4aac-bd68-4ccad207ae36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you are running on Saint Peters' Databricks, then use the following line. If not, you need to adjust the file path to a local one.\n",
    "from pyspark.sql.types import StringType, StructType, DateType, FloatType, StructField\n",
    "fields = [ StructField('Date', DateType(), True),\n",
    "    StructField('Open', FloatType(), True),\n",
    "    StructField('High', FloatType(), True),\n",
    "    StructField('Low', FloatType(), True),\n",
    "    StructField('Close', FloatType(), True),\n",
    "    StructField('Volume', FloatType(), True),\n",
    "    StructField('Adj Close', FloatType(), True) ]\n",
    "schema = StructType(fields)\n",
    "df = spark.read.csv('dbfs:/FileStore/shared_uploads/dlee5@saintpeters.edu/ds610/walmart_stock.csv', schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3688025f-9ca8-4034-8b05-d38ec0f6c319",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 1: Warmup\n",
    "Display the schema of the loaded DataFrame `df` via `printSchema` method.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.printSchema.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05bbec3-49bf-4fac-9259-4fe50e1b57d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Date: date (nullable = true)\n |-- Open: float (nullable = true)\n |-- High: float (nullable = true)\n |-- Low: float (nullable = true)\n |-- Close: float (nullable = true)\n |-- Volume: float (nullable = true)\n |-- Adj Close: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Your code for Part 1 goes here.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9893a8c5-14dd-4001-a1b8-5e7f4bc09d51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2: Warmup\n",
    "Display the first 15 rows of the DataFrame `df` via `show` method.\n",
    "\n",
    "https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.show.html\n",
    "\n",
    "As you will see, this is a time-stamped data with each data containing the opening price, the high price, the low price, the closing price with the trading volume. For our assignment `Adj Close` (adjusted closing price) is not considered. For more information:\n",
    "https://www.investopedia.com/terms/a/adjusted_closing_price.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e41d552c-3f28-4160-82a5-d3b2a6045ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+---------+---------+\n|      Date| Open| High|  Low|Close|   Volume|Adj Close|\n+----------+-----+-----+-----+-----+---------+---------+\n|2012-01-03|59.97|61.06|59.87|60.33|1.26688E7|52.619236|\n|2012-01-04|60.21|60.35|59.47|59.71|9593300.0|52.078476|\n|2012-01-05|59.35|59.62|58.37|59.42|1.27682E7| 51.82554|\n|2012-01-06|59.42|59.45|58.87| 59.0|8069400.0| 51.45922|\n|2012-01-09|59.03|59.55|58.92|59.18|6679300.0|51.616215|\n|2012-01-10|59.43|59.71|58.98|59.04|6907300.0| 51.49411|\n|2012-01-11|59.06|59.53|59.04| 59.4|6365600.0|51.808098|\n|2012-01-12|59.79| 60.0| 59.4| 59.5|7236400.0|51.895317|\n|2012-01-13|59.18|59.61|59.01|59.54|7729300.0|51.930202|\n|2012-01-17|59.87|60.11|59.52|59.85|8500000.0| 52.20058|\n|2012-01-18|59.79|60.03|59.65|60.01|5911400.0| 52.34013|\n|2012-01-19|59.93|60.73|59.75|60.61|9234600.0|52.863445|\n|2012-01-20|60.75|61.25|60.67|61.01|1.03788E7|53.212322|\n|2012-01-23|60.81|60.98|60.51|60.91|7134100.0|53.125103|\n|2012-01-24|60.75| 62.0|60.75|61.39|7362800.0|53.543755|\n+----------+-----+-----+-----+-----+---------+---------+\nonly showing top 15 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Your code for Part 2 goes here.\n",
    "df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ca0eb57-becc-487c-9d78-9f99a6d81f85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 3: Apply Manipulation on DataFrame\n",
    "Next, we decide that we would like to analyze data on the per-month basis and it would be helpful to have a column named `Month` in the DataFrame.\n",
    "\n",
    "Write a code that creates another DataFrame `df2` with the same schema as `df` but an additional column called `Month`. For example, for a row in `df` with `Date=2012-01-03`, `Month` should be 1.\n",
    "\n",
    "Although you can do this in many ways, it is recommended to use the `select` command:\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html\n",
    "\n",
    "After you create `df2`, run `show` on it to quickly check whether it is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c0873b-8aa6-428f-a413-c6350f7dc435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+---------+---------+-----+\n|      Date| Open| High|  Low|Close|   Volume|Adj Close|Month|\n+----------+-----+-----+-----+-----+---------+---------+-----+\n|2012-01-03|59.97|61.06|59.87|60.33|1.26688E7|52.619236|    1|\n|2012-01-04|60.21|60.35|59.47|59.71|9593300.0|52.078476|    1|\n|2012-01-05|59.35|59.62|58.37|59.42|1.27682E7| 51.82554|    1|\n|2012-01-06|59.42|59.45|58.87| 59.0|8069400.0| 51.45922|    1|\n|2012-01-09|59.03|59.55|58.92|59.18|6679300.0|51.616215|    1|\n|2012-01-10|59.43|59.71|58.98|59.04|6907300.0| 51.49411|    1|\n|2012-01-11|59.06|59.53|59.04| 59.4|6365600.0|51.808098|    1|\n|2012-01-12|59.79| 60.0| 59.4| 59.5|7236400.0|51.895317|    1|\n|2012-01-13|59.18|59.61|59.01|59.54|7729300.0|51.930202|    1|\n|2012-01-17|59.87|60.11|59.52|59.85|8500000.0| 52.20058|    1|\n|2012-01-18|59.79|60.03|59.65|60.01|5911400.0| 52.34013|    1|\n|2012-01-19|59.93|60.73|59.75|60.61|9234600.0|52.863445|    1|\n|2012-01-20|60.75|61.25|60.67|61.01|1.03788E7|53.212322|    1|\n|2012-01-23|60.81|60.98|60.51|60.91|7134100.0|53.125103|    1|\n|2012-01-24|60.75| 62.0|60.75|61.39|7362800.0|53.543755|    1|\n|2012-01-25|61.18|61.61|61.04|61.47|5915800.0| 53.61353|    1|\n|2012-01-26| 61.8|61.84|60.77|60.97|7436200.0|53.177437|    1|\n|2012-01-27|60.86|61.12|60.54|60.71|6287300.0|52.950665|    1|\n|2012-01-30|60.47|61.32|60.35| 61.3|7636900.0|53.465256|    1|\n|2012-01-31|61.53|61.57|60.58|61.36|9761500.0| 53.51759|    1|\n+----------+-----+-----+-----+-----+---------+---------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Your code for Part 3 goes here.\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "df2 = df.select('*', month(df.Date).alias('Month'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49933ae7-58f9-40c9-aa29-e25dfdbb5727",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Interlude\n",
    "The following code block will stop you from proceeding to the rest of the parts until the parts above are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae39539-93fd-4ca8-83e1-c91835b4e7de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not(df2 and type(df2) is DataFrame and 'Month' in df2.columns):\n",
    "    raise ValueError(\"You will need to finish Part 1 before moving onto Part 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce51ff91-7b2d-4739-941c-137db611d96d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Date: date (nullable = true)\n |-- Open: float (nullable = true)\n |-- High: float (nullable = true)\n |-- Low: float (nullable = true)\n |-- Close: float (nullable = true)\n |-- Volume: float (nullable = true)\n |-- Adj Close: float (nullable = true)\n |-- Month: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72e78b97-2c64-46fb-850b-972ddc6f18fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 4\n",
    "We would like to find out for each month, the minimum, the average, and maximum closing prices.\n",
    "\n",
    "Your task is to write the code to find out these information. You may find `groupBy` and `agg` syntax useful:\n",
    "https://spark.apache.org/docs/latest/api/python//reference/pyspark.pandas/api/pyspark.pandas.groupby.DataFrameGroupBy.agg.html#pyspark.pandas.groupby.DataFrameGroupBy.agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c31f5ea-a382-4da6-b3db-a16861ce6e90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+------------------+-----------------+\n|Month|Min_Closing_Price| Avg_Closing_Price|Max_Closing_Price|\n+-----+-----------------+------------------+-----------------+\n|    1|             59.0|60.235499954223634|            61.47|\n|    2|            58.46|60.897999954223636|            62.48|\n|    3|            58.82|60.433636752041906|            61.23|\n|    4|            57.36| 60.14900016784668|            62.45|\n|    5|             58.7| 60.54388851589627|            65.07|\n+-----+-----------------+------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your code for Part 4 goes here.\n",
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "\n",
    "df2.groupBy(\"Month\").agg(min(\"Close\").alias('Min_Closing_Price'),avg(\"Close\").alias('Avg_Closing_Price'),max(\"Close\").alias('Max_Closing_Price')).orderBy(\"Month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40040eda-c5a3-49c2-b8f9-020a5a514229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 5\n",
    "Now let us do Part 4 but with SQL. The first thing we will do is to create a temporary table view on `df2`. This is done below for you.\n",
    "\n",
    "Your task is now to write a SQL `SELECT` statement on the table `walmart_stock_price_history` to get the same results as Part 4. For documentation, see:\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-getting-started.html#running-sql-queries-programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd5d37d-f37f-4bb1-bc29-9de83867403f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Temporary table in PySpark. DO NOT MODIFY.\n",
    "df2.createOrReplaceTempView(\"walmart_stock_price_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31cf516b-2f9d-43b5-9615-bc7bc993e8df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+------------------+-----------------+\n|Month|Min_Closing_Price| Avg_Closing_Price|Max_Closing_Price|\n+-----+-----------------+------------------+-----------------+\n|    1|             59.0|60.235499954223634|            61.47|\n|    2|            58.46|60.897999954223636|            62.48|\n|    3|            58.82|60.433636752041906|            61.23|\n|    4|            57.36| 60.14900016784668|            62.45|\n|    5|             58.7| 60.54388851589627|            65.07|\n+-----+-----------------+------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Your code for Part 5 goes here.\n",
    "\n",
    "spark.sql(\"select Month,min(Close) as Min_Closing_Price ,avg(Close) as Avg_Closing_Price,max(Close) as Max_Closing_Price from walmart_stock_price_history group by Month order by Month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd8623be-e87a-45d4-9c5e-e01a6f5260e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 6 (Extra Credit)\n",
    "Now let us practice UDFs (https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.udf.html).\n",
    "\n",
    "UDF's (User Defined Functions) support re-using transformations as a function which can registered in Spark environment.\n",
    "\n",
    "Your task is to compute for each month, the median trading volume when the closing price is higher than the opening price. Note that this part has a slight *twist* - if you end up spending a lot of time on this part, you may want to skip.\n",
    "\n",
    "For helpful examples, you may also want to look at:\n",
    "https://danvatterott.com/blog/2018/09/06/python-aggregate-udfs-in-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94eae5db-9e2b-47af-8145-5accfb2ad2bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n|Month|Median_Trading_Volume|\n+-----+---------------------+\n|    1|            9234600.0|\n|    2|            5173100.0|\n|    3|            8251900.0|\n|    4|            7957300.0|\n|    5|            2.92924E7|\n+-----+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Fill out your code for Part 6 in this box.\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import statistics\n",
    "\n",
    "# Your code for median volume computation goes here. The input to this function \n",
    "# would be a list of trading volumes for each month when the closing price is \n",
    "# higher than the opening price.\n",
    "def compute_median_volume(x):\n",
    "    \n",
    "    # IMPLEMENT ME FOR PART 6\n",
    "    return float(x[int(len(x) / 2)])\n",
    "\n",
    "# This code registers the function as a UDF.\n",
    "compute_median_volume_udf = F.udf(compute_median_volume, FloatType())\n",
    "\n",
    "# Write your code for performing the aggregation to display the median price for\n",
    "# each month for days whose closing price > opening price.\n",
    "\n",
    "df2.filter(df2.Close > df2.Open).groupBy('Month').agg(compute_median_volume_udf(F.expr(\"collect_list(Volume)\")).alias(\"Median_Trading_Volume\")).orderBy('Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15917e3c-1f9b-4b75-a8a7-aa3d53edc3e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 7\n",
    "What is your favorite way of performing aggregations (`groupBy` syntax, `createOrReplaceTempView` then SQL, `groupby` aggregation with UDF)? Did you spend a lot of time on Part 6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc46c98-da26-4c84-81e6-9851f1f2e185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response to Part 5: My favorite way of performing aggregations is by using createOrReplaceTempView, then SQL. In this way, I was able to achieve the aggregation a bit fast as it is easy to understand, and groupBy syntax is also an easy way to do the aggregation, but UDF took some time for me as I have good knowledge of SQL, but this UDF is a new concept where I have spent some time to understand the concept. After that, I have implemented my learning and achieved the result.\n"
     ]
    }
   ],
   "source": [
    "# Your response to Part 5 goes here.\n",
    "print(\"Your response to Part 5: My favorite way of performing aggregations is by using createOrReplaceTempView, then SQL. In this way, I was able to achieve the aggregation a bit fast as it is easy to understand, and groupBy syntax is also an easy way to do the aggregation, but UDF took some time for me as I have good knowledge of SQL, but this UDF is a new concept where I have spent some time to understand the concept. After that, I have implemented my learning and achieved the result.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ds610_week_03_hw_starter_code",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
